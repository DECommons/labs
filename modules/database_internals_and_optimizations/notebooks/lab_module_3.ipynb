{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fb49b8d",
   "metadata": {},
   "source": [
    "# Module 3: The Container (File Formats)\n",
    "**Goal**: Shatter the illusion that \"data is just text.\" We will prove that how you package bytes (Text vs. Binary, Row vs. Column) dramatically impacts speed and storage, dictated by the physics of CPU parsing and I/O alignment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b790ac4",
   "metadata": {},
   "source": [
    "### 1. Setup and Initialization\n",
    "First, let's load our physics lab tools. We will use pandas for handling general data, duckdb for high-performance file introspection, and fastavro/pyarrow to manipulate binary formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12487c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import duckdb\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import fastavro\n",
    "import pyarrow as pa\n",
    "import pyarrow.csv as pa_csv\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure Visualization\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
    "\n",
    "# Define Data Paths (From our pre-loaded Universe)\n",
    "DATA_DIR = \"../data/\"\n",
    "CSV_FILE = f\"{DATA_DIR}users.csv\"\n",
    "PARQUET_FILE = f\"{DATA_DIR}users.parquet\"\n",
    "\n",
    "# Create a JSON version of users for the experiment\n",
    "print(\"Generating lab data...\")\n",
    "df = pd.read_csv(CSV_FILE)\n",
    "JSON_FILE = f\"{DATA_DIR}users.json\"\n",
    "df.to_json(JSON_FILE, orient=\"records\", lines=True)\n",
    "\n",
    "print(\"Setup Complete. Lab is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3610fe01",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbe2237",
   "metadata": {},
   "source": [
    "### 2. Experiment 3.1: The Cost of Text (Schema-on-Read)\n",
    "**The Physics**: Humans love text (CSV/JSON) because we can read it. Computers hate text. When a database reads a CSV, it must perform Parsing:\n",
    "\n",
    "1. Read a byte.\n",
    "2. Is it a comma? If no, keep buffering.\n",
    "3. Is it a newline? If yes, end the row.\n",
    "4. Convert the string \"12345\" into the integer 12345 (ASCII-to-Integer conversion).\n",
    "\n",
    "Binary formats (like Parquet) map directly to memory. The computer doesn't \"read\" the number; it just copies the bytes.\n",
    "\n",
    "**Hypothesis**: Will reading a binary format (Parquet) be significantly faster than reading text formats (CSV/JSON), even for the exact same data?\n",
    "\n",
    "**The Experiment**: We will time how long it takes to load the users dataset into memory using three different formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bf5f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the experiment function\n",
    "def benchmark_read(file_path, reader_func, format_name):\n",
    "    start_time = time.time()\n",
    "    # Force a materialization to memory (list or df) to trigger the read\n",
    "    _ = reader_func(file_path)\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "# 2. Run the experiment\n",
    "results = {}\n",
    "\n",
    "# Measure CSV (Text)\n",
    "results['CSV'] = benchmark_read(CSV_FILE, pd.read_csv, 'CSV')\n",
    "\n",
    "# Measure JSON (Text + Structure overhead)\n",
    "results['JSON'] = benchmark_read(JSON_FILE, lambda f: pd.read_json(f, lines=True), 'JSON')\n",
    "\n",
    "# Measure Parquet (Binary + Columnar)\n",
    "results['Parquet'] = benchmark_read(PARQUET_FILE, pd.read_parquet, 'Parquet')\n",
    "\n",
    "# 3. Visualize\n",
    "plt.bar(results.keys(), results.values(), color=['salmon', 'orange', 'skyblue'])\n",
    "plt.title(\"Read Time: Text vs. Binary (Lower is Better)\")\n",
    "plt.ylabel(\"Time (Seconds)\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"CSV Time: {results['CSV']:.4f}s\")\n",
    "print(f\"Parquet Time: {results['Parquet']:.4f}s\")\n",
    "print(f\"Speedup Factor: {results['CSV'] / results['Parquet']:.1f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93702a10",
   "metadata": {},
   "source": [
    "**The Conclusion**: You likely observed that Parquet was the fastest and that JSON was the slowest.\n",
    "- **CSV/JSON (Schema-on-Read)**: The CPU spent most of its time decoding text (finding commas, converting \"1\" \"0\" \"0\" to integer 100). This is CPU-bound.\n",
    "- **Parquet (Schema-on-Write)**: The schema is stored in the file header. The engine knows exactly how many bytes to grab for an integer. It simply `memcpy` (memory copy) the data from disk to RAM.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67a676e",
   "metadata": {},
   "source": [
    "### 3. Experiment 3.2: The Power of Metadata (Data Skipping)\n",
    "**The Physics**: In a CSV, if you want to find orders from \"2023-01-01\", you must scan the file from top to bottom because the file has no \"Brain\"â€”it's just text.\n",
    "\n",
    "Columnar formats like Parquet divide rows into Row Groups. Each group has a Footer containing statistics (Min/Max values) for the columns in that chunk. If you ask for WHERE order_date = '2025-01-01', the database looks at the footer first. If the footer says \"This chunk contains dates from 2020 to 2022\", the database skips the entire chunk without reading the data.\n",
    "\n",
    "**Hypothesis**: If we filter for a value that exists at the very end of the file, Parquet will be instant because it skips the early data, while CSV will be slow because it must scan everything.\n",
    "\n",
    "**The Experiment**: We will use the orders_sorted.csv data. Since it is sorted by date, Parquet's Min/Max statistics will be perfectly efficient (chunks will not overlap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34ed6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare Data: Convert sorted CSV to Parquet with small Row Groups\n",
    "# We force small row groups to create many \"chunks\" for the demo\n",
    "print(\"Preparing partitioned Parquet file...\")\n",
    "table = pa.csv.read_csv(f\"{DATA_DIR}orders_sorted.csv\")\n",
    "pq.write_table(table, f\"{DATA_DIR}orders_sorted_chunked.parquet\", row_group_size=10000)\n",
    "\n",
    "# 2. Define the Query\n",
    "# We pick a date that we know is at the END of the file\n",
    "target_date = \"2023-12-31\" \n",
    "query_csv = f\"SELECT * FROM '{DATA_DIR}orders_sorted.csv' WHERE order_date = '{target_date}'\"\n",
    "query_parquet = f\"SELECT * FROM '{DATA_DIR}orders_sorted_chunked.parquet' WHERE order_date = '{target_date}'\"\n",
    "\n",
    "# 3. Run the Experiment\n",
    "times = {}\n",
    "\n",
    "# CSV Scan\n",
    "start = time.time()\n",
    "duckdb.sql(query_csv).fetchall()\n",
    "times['CSV (Scan)'] = time.time() - start\n",
    "\n",
    "# Parquet Scan (Smart)\n",
    "start = time.time()\n",
    "duckdb.sql(query_parquet).fetchall()\n",
    "times['Parquet (Pruning)'] = time.time() - start\n",
    "\n",
    "# 4. Visualize\n",
    "plt.bar(times.keys(), times.values(), color=['red', 'green'])\n",
    "plt.title(\"Query Time: Full Scan vs. Partition Pruning\")\n",
    "plt.ylabel(\"Time (Seconds)\")\n",
    "plt.show()\n",
    "\n",
    "# Bonus: Show the Metadata that made this possible\n",
    "print(\"\\n--- The Cheat Sheet (Parquet Metadata) ---\")\n",
    "print(duckdb.sql(f\"SELECT row_group_id, stats_min, stats_max FROM parquet_metadata('{DATA_DIR}orders_sorted_chunked.parquet') WHERE path_in_schema = 'order_date' LIMIT 5\").df())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d372c5",
   "metadata": {},
   "source": [
    "**The Conclusion**: The Parquet query was likely near-instant, while the CSV query took measurable time.\n",
    "- **The Physics**: The CSV engine had to parse 500,000 dates to check if they matched. The Parquet engine read the Metadata Footer first. It saw that the first 49 chunks had `MAX(order_date) < 2023-12-31`, so it physically did not read those bytes from the disk. It only jumped to the last chunk.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed190d3",
   "metadata": {},
   "source": [
    "### 4. Experiment 3.3: Serialization (Row-Based vs. Column-Based Writing)\n",
    "**The Physics**: Not all binary is created equal.\n",
    "- **Avro (Row-Based Binary)**: Stores data physically as [Row1_Bytes][Row2_Bytes]. This is perfect for Streaming (Kafka) because you can write one row at a time instantly.\n",
    "- **Parquet (Column-Based Binary)**: Stores data physically as [Col1_All_Rows][Col2_All_Rows]. To write a Parquet file, you must buffer many rows in memory, pivot them, and then write the block. It is terrible for writing one row at a time.\n",
    "\n",
    "**Hypothesis**: Simulating a streaming producer (writing 1 row at a time) will be much faster with Avro/Row-based formats than Parquet.\n",
    "\n",
    "**The Experiment**: We will simulate a \"Log Producer\" writing 1,000 events, one by one, to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f135e3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup Dummy Data (1000 records)\n",
    "records = [{\"id\": i, \"status\": \"active\", \"ts\": time.time()} for i in range(1000)]\n",
    "schema = {\n",
    "    \"type\": \"record\", \"name\": \"User\", \"fields\": [\n",
    "        {\"name\": \"id\", \"type\": \"int\"},\n",
    "        {\"name\": \"status\", \"type\": \"string\"},\n",
    "        {\"name\": \"ts\", \"type\": \"double\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 2. Define Writers\n",
    "\n",
    "def write_avro_stream():\n",
    "    # Avro NATIVELY supports streaming.\n",
    "    # We pass the list (iterator), and it writes Row 1, then Row 2, etc. efficiently.\n",
    "    with open(f\"{DATA_DIR}stream.avro\", \"wb\") as out:\n",
    "        fastavro.writer(out, schema, records)\n",
    "\n",
    "def write_parquet_stream():\n",
    "    # Parquet DOES NOT support streaming rows naturally.\n",
    "    # To simulate a live stream (writing data as soon as it arrives),\n",
    "    # we are forced to convert EVERY row into a mini-table and write it.\n",
    "    # This simulates the \"Overhead\" of using a Columnar format for real-time data.\n",
    "    for record in records:\n",
    "        table = pa.Table.from_pylist([record])\n",
    "        pq.write_table(table, f\"{DATA_DIR}stream_temp.parquet\")\n",
    "\n",
    "# 3. Run Benchmark\n",
    "stream_times = {}\n",
    "\n",
    "print(\"Streaming Avro (Row-by-Row)...\")\n",
    "start = time.time()\n",
    "write_avro_stream()\n",
    "stream_times['Avro (Row)'] = time.time() - start\n",
    "\n",
    "print(\"Streaming Parquet (Forced Row-by-Row)...\")\n",
    "start = time.time()\n",
    "write_parquet_stream()\n",
    "stream_times['Parquet (Col)'] = time.time() - start\n",
    "\n",
    "# 4. Visualize\n",
    "# We use Log Scale because the difference is usually massive\n",
    "plt.bar(stream_times.keys(), stream_times.values(), color=['gold', 'purple'])\n",
    "plt.yscale('log') \n",
    "plt.title(\"Streaming Write Speed: 1000 Rows (Log Scale)\")\n",
    "plt.ylabel(\"Time (Seconds)\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Avro Time: {stream_times['Avro (Row)']:.4f}s\")\n",
    "print(f\"Parquet Time: {stream_times['Parquet (Col)']:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34ff916",
   "metadata": {},
   "source": [
    "**The Conclusion**: Parquet is likely orders of magnitude slower here.\n",
    "- **Avro**: Is a \"Streaming\" format. It dumps the bytes for the row and moves on.\n",
    "- **Parquet**: Is a \"Storage\" format. For every single write, it has to calculate metadata, encode columns, and organize headers. It is too heavy for real-time row processing.\n",
    "- **Lesson**: Use Avro/Protobuf for Moving data (Kafka/APIs). Use Parquet/ORC for Analyzing data (Data Lakes).\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
