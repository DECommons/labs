{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c48910b1",
   "metadata": {},
   "source": [
    "# Module 2: Row vs. Column Stores\n",
    "**Goal**: Shatter the illusion that \"Data is Data.\"\n",
    "The physical layout of bytes on a disk determines whether your database is a Ferrari or a Dump Truck. In this chapter, we explore the two fundamental religions of database physics: **Row-Oriented** (The General Practitioner) and **Column-Oriented** (The Specialist).\n",
    "- **Row Stores (Postgres, MySQL)**: Keep all data for one person together.\n",
    "- **Column Stores (DuckDB, Snowflake, Parquet)**: Keep all data for one attribute together.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0437da2a",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "First, we establish connections to our two laboratory subjects:\n",
    "1. **Postgres**: Represents the traditional Row Store (OLTP).\n",
    "2. **DuckDB**: Represents the modern Column Store (OLAP).\n",
    "\n",
    "We will load the `users.csv` and `orders_sorted.csv` datasets into both engines to ensure a fair fight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af3cc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configure Visualization\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "\n",
    "# --- 1. Connect to DBs ---\n",
    "pg_conn = psycopg2.connect(host=\"db_int_opt\", user=\"admin\", password=\"password\", dbname=\"db_int_opt\")\n",
    "pg_cursor = pg_conn.cursor()\n",
    "duck_conn = duckdb.connect()\n",
    "\n",
    "# --- 2. The Smart Loader Function ---\n",
    "def smart_load_to_postgres(file_path, table_name):\n",
    "    \"\"\"\n",
    "    Uses DuckDB to infer the CSV schema, then generates the Postgres CREATE TABLE \n",
    "    statement dynamically. This prevents 'Schema Mismatch' errors.\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ•µï¸â€â™‚ï¸ Inspecting {table_name} schema via DuckDB...\")\n",
    "    \n",
    "    # Use DuckDB to read the file and get data types\n",
    "    # We load 1 row just to get the columns\n",
    "    df_schema = duck_conn.execute(f\"SELECT * FROM read_csv_auto('{file_path}') LIMIT 1\").df()\n",
    "    \n",
    "    # Map Pandas/DuckDB types to Postgres Types\n",
    "    col_defs = []\n",
    "    for col, dtype in df_schema.dtypes.items():\n",
    "        pg_type = \"TEXT\" # Default\n",
    "        if \"int\" in str(dtype): pg_type = \"BIGINT\"\n",
    "        elif \"float\" in str(dtype): pg_type = \"FLOAT\"\n",
    "        elif \"datetime\" in str(dtype): pg_type = \"TIMESTAMP\"\n",
    "        elif \"object\" in str(dtype) and \"date\" in col: pg_type = \"DATE\" # Heuristic for dates\n",
    "        elif \"bool\" in str(dtype): pg_type = \"BOOLEAN\"\n",
    "        \n",
    "        col_defs.append(f'\"{col}\" {pg_type}') # Quote cols to handle reserved words\n",
    "    \n",
    "    create_sql = f\"CREATE TABLE {table_name} ({', '.join(col_defs)});\"\n",
    "    \n",
    "    # Execute in Postgres\n",
    "    pg_cursor.execute(f\"DROP TABLE IF EXISTS {table_name};\")\n",
    "    pg_cursor.execute(create_sql)\n",
    "    \n",
    "    # Load Data\n",
    "    with open(file_path, 'r') as f:\n",
    "        next(f) # Skip header\n",
    "        pg_cursor.copy_expert(f\"COPY {table_name} FROM STDIN WITH CSV\", f)\n",
    "    pg_conn.commit()\n",
    "    print(f\"âœ… Loaded {table_name} successfully into Postgres.\")\n",
    "\n",
    "# --- 3. Execute Loading ---\n",
    "print(\"â³ Starting Robust Data Load...\")\n",
    "users_file = \"../data/users.csv\"\n",
    "orders_file = \"../data/orders_sorted.csv\"\n",
    "\n",
    "try:\n",
    "    # Load Postgres (Row Store)\n",
    "    smart_load_to_postgres(users_file, \"users\")\n",
    "    smart_load_to_postgres(orders_file, \"orders\")\n",
    "    \n",
    "    # Load DuckDB (Column Store)\n",
    "    duck_conn.execute(f\"CREATE OR REPLACE TABLE users AS SELECT * FROM read_csv_auto('{users_file}')\")\n",
    "    duck_conn.execute(f\"CREATE OR REPLACE TABLE orders AS SELECT * FROM read_csv_auto('{orders_file}')\")\n",
    "    print(\"ðŸŽ‰ All Data Loaded Successfully.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    pg_conn.rollback()\n",
    "    print(f\"âŒ Critical Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8797407d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c32c15",
   "metadata": {},
   "source": [
    "## Experiment 2.1: The OLTP Model (The Card Catalog)\n",
    "**The Concept**: In a **Row Store**, data is stored like a card catalog or a Rolodex. If you want to know everything about \"User 101\", the database grabs one \"card\" (Page) and has the ID, Name, Email, and Status right there in a single contiguous block of memory.\n",
    "\n",
    "In a **Column Store**, that same user's data is scattered across different memory regions. To rebuild the \"User 101\" object, the database must jump to the \"ID List\", then the \"Name List\", then the \"Email List\" and stitch them together (Tuple Reconstruction).\n",
    "\n",
    "**Hypothesis**: Postgres (Row) should be faster (or at least highly competitive) at fetching a single, complete record compared to DuckDB (Column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eda69c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# --- Step 1: Define the Query (Fetch a single entity) ---\n",
    "# We use a random ID. We need to check the actual column name for ID from the load above.\n",
    "# Based on typical generation, it's likely 'user_id' or 'id'.\n",
    "id_col = \"user_id\" \n",
    "target_id = 5000  \n",
    "\n",
    "query = f\"SELECT * FROM users WHERE {id_col} = {target_id}\"\n",
    "\n",
    "# --- Step 2: Measure Postgres (Row Store) ---\n",
    "start_time = time.time()\n",
    "for _ in range(100): # Run 100 times to average out noise\n",
    "    pg_cursor.execute(query)\n",
    "    result = pg_cursor.fetchone()\n",
    "pg_duration = (time.time() - start_time) / 100\n",
    "\n",
    "# --- Step 3: Measure DuckDB (Column Store) ---\n",
    "start_time = time.time()\n",
    "for _ in range(100):\n",
    "    duck_conn.execute(query).fetchone()\n",
    "duck_duration = (time.time() - start_time) / 100\n",
    "\n",
    "# --- Step 4: Visualize ---\n",
    "print(f\"Postgres (Row) Time: {pg_duration:.6f}s\")\n",
    "print(f\"DuckDB (Col) Time:   {duck_duration:.6f}s\")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(x=['Postgres (Row Store)', 'DuckDB (Column Store)'], y=[pg_duration, duck_duration], palette=\"viridis\")\n",
    "plt.title(\"Time to Fetch ONE Complete Row (SELECT * WHERE ID=X)\")\n",
    "plt.ylabel(\"Execution Time (Seconds)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83bacd1",
   "metadata": {},
   "source": [
    "### Observation & Conclusion: The \"Embedded\" Anomaly\n",
    "Your graph likely shows that **DuckDB** (Column Store) was significantly faster, which contradicts the classic textbook rule that Row Stores are better for single-row lookups.\n",
    "\n",
    "**Why did the \"wrong\" database win?** You have just discovered the cost of **Network Latency vs. In-Process Execution**.\n",
    "- **The Hidden Cost (Postgres)**: Postgres runs as a separate Service (Client-Server). Even though it found the row instantly on disk, it had to package that data, send it over a TCP network port (to Python), and wait for Python to unpack it. That \"commute\" took 90% of the time.\n",
    "- **The In-Memory Advantage (DuckDB)**: DuckDB runs Embedded inside your Python process. It has zero network overhead. It grabs the data directly from RAM.\n",
    "- **The Physics is still true**: DuckDB did have to perform Tuple Reconstruction (stitching the Name, Email, and ID vectors back together), which is inefficient. However, the penalty for that was tiny compared to the \"Network Tax\" Postgres paid.\n",
    "\n",
    "**Key Takeaway**: In a local lab, architecture (Embedded vs. Server) often outweighs storage physics (Row vs. Column). If this were a real cloud app with both DBs on a remote server, Postgres would likely win this test.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d3aae6",
   "metadata": {},
   "source": [
    "## Experiment 2.2: The OLAP Model (The Specialist)\n",
    "**The Concept**: We want to perform a heavy math operation over the entire dataset.\n",
    "- **Row Store (Postgres)**: Must read every row (ID, User, Product, Date) just to extract the `quantity` integer. It reads mostly \"waste.\"\n",
    "- **Column Store (DuckDB)**: Reads *only* the `quantity` column. It ignores the IDs and Dates entirely.\n",
    "\n",
    "**Hypothesis**: DuckDB should significantly outperform Postgres because it touches less data on the disk/memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56533523-1dfb-493e-9695-07b24c70b0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Experiment 2.2: The OLAP Model (Corrected) ---\n",
    "import time\n",
    "\n",
    "# 1. Define the Query (Math over the whole dataset)\n",
    "# We use \"quantity\" because we now know it exists!\n",
    "col_name = \"quantity\"\n",
    "query = f'SELECT AVG({col_name}) FROM orders'\n",
    "\n",
    "# 2. Measure Postgres (Row Store)\n",
    "start_time = time.time()\n",
    "for _ in range(10): # Run 10 times to smooth out variance\n",
    "    pg_cursor.execute(query)\n",
    "    pg_cursor.fetchone()\n",
    "pg_agg_duration = (time.time() - start_time) / 10\n",
    "\n",
    "# 3. Measure DuckDB (Column Store)\n",
    "start_time = time.time()\n",
    "for _ in range(10):\n",
    "    duck_conn.execute(query).fetchone()\n",
    "duck_agg_duration = (time.time() - start_time) / 10\n",
    "\n",
    "# 4. Visualize\n",
    "print(f\"Postgres (Row) Time: {pg_agg_duration:.6f}s\")\n",
    "print(f\"DuckDB (Col) Time:   {duck_agg_duration:.6f}s\")\n",
    "\n",
    "# Calculate Speedup \n",
    "speedup = pg_agg_duration / duck_agg_duration if duck_agg_duration > 0 else 0\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(x=['Postgres (Row Store)', 'DuckDB (Column Store)'], y=[pg_agg_duration, duck_agg_duration], palette=\"magma\")\n",
    "plt.title(f\"Time to Calculate AVG(quantity) - {speedup:.1f}x Speedup\")\n",
    "plt.ylabel(\"Execution Time (Seconds)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3cf911",
   "metadata": {},
   "source": [
    "### Observation\n",
    "You should see a clear win for DuckDB here.\n",
    "- **Why?** DuckDB loaded a single contiguous array of integers (`quantity`) and used CPU Vector instructions (SIMD) to sum them up. Postgres had to jump through row headers and pointers for every single record.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9ddc74",
   "metadata": {},
   "source": [
    "## Experiment 2.3: Compression (The Squeeze)\n",
    "**The Concept**: Column stores compress better because data of the same type is stored together.\n",
    "- **Users Table**: We previously verified (via `users` file inspection) that `account_status` exists. This is a low-cardinality text column (e.g., mostly \"Active\" or \"Inactive\"), which is perfect for **Run-Length Encoding (RLE)**.\n",
    "\n",
    "**Hypothesis**: The Columnar Parquet file will be drastically smaller than the Row-based CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce81e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- Step 1: Prepare the Data ---\n",
    "# We grab the 'users' table which we successfully loaded earlier\n",
    "df = duck_conn.execute(\"SELECT * FROM users ORDER BY account_status\").df()\n",
    "\n",
    "# --- Step 2: Write as CSV (Row-oriented, uncompressed text) ---\n",
    "csv_path = \"../data/experiment_users.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "# --- Step 3: Write as Parquet (Column-oriented, Snappy compressed) ---\n",
    "parquet_path = \"../data/experiment_users.parquet\"\n",
    "df.to_parquet(parquet_path, engine='pyarrow', compression='snappy')\n",
    "\n",
    "# --- Step 4: Measure Sizes ---\n",
    "csv_size = os.path.getsize(csv_path) / 1024 / 1024 # MB\n",
    "parquet_size = os.path.getsize(parquet_path) / 1024 / 1024 # MB\n",
    "\n",
    "# --- Step 5: Visualize ---\n",
    "print(f\"CSV Size:     {csv_size:.2f} MB\")\n",
    "print(f\"Parquet Size: {parquet_size:.2f} MB\")\n",
    "reduction = (1 - (parquet_size / csv_size)) * 100\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(x=['CSV (Row)', 'Parquet (Column)'], y=[csv_size, parquet_size], palette=\"Blues_d\")\n",
    "plt.title(f\"Storage Footprint: {reduction:.1f}% Reduction\")\n",
    "plt.ylabel(\"File Size (MB)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5433521d",
   "metadata": {},
   "source": [
    "### Observation & Conclusion\n",
    "The Parquet file is likely greater than 50% smaller than the CSV.\n",
    "- **Homogeneity**: Compression algorithms love repetition. In a Column store, the `account_status` column looks like `['Active', 'Active', 'Active'...]`. This compresses to almost nothing.\n",
    "- **Type Awareness**: The database knows the `age` column is Integers. It doesn't store them as text characters (\"2\", \"5\"), but as tight binary integers.\n",
    "\n",
    "**Final Lesson**: Use **Row Stores** (Postgres) when you are building an app that manages individual entities (Users, Orders, Carts). Use **Column Stores** (DuckDB, Snowflake) when you are analyzing trends across populations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
