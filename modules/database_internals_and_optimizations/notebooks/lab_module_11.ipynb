{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "991f7df1",
   "metadata": {},
   "source": [
    "# Module 11: Distributing the Load\n",
    "**Goal**: Shatter the illusion of \"Infinite Scalability.\" The Lesson: When you add more computers, you don't just get more power; you get a \"Network Tax.\" We will demonstrate why Shuffles kill performance and how Skew creates \"straggler\" nodes that slow down the entire cluster.\n",
    "\n",
    "## 1. Setup and \"Cluster\" Initialization\n",
    "First, we load the heavy data (`clickstream.parquet`) and the lookup data (`users.parquet`). We will then define a helper function to simulate a 4-Node Cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4b476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import duckdb\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import hashlib\n",
    "import sys\n",
    "\n",
    "# Configure Layout\n",
    "plt.style.use('seaborn-v0_8')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"--- 1. CONNECTING TO DATA ---\")\n",
    "# Load the \"Justin Bieber\" Skew dataset (2M rows)\n",
    "# If this file doesn't exist from previous chapters, we generate a mock version here for safety.\n",
    "try:\n",
    "    df_clicks = pd.read_parquet('../data/clickstream.parquet')\n",
    "    print(f\"Loaded Clickstream: {len(df_clicks):,} rows (The 'Big' Table)\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Generating Clickstream Data...\")\n",
    "    # 50% of rows are User ID 1 (The Skew), the rest are random\n",
    "    ids = np.concatenate([np.ones(1_000_000), np.random.randint(2, 100000, 1_000_000)])\n",
    "    np.random.shuffle(ids)\n",
    "    df_clicks = pd.DataFrame({'user_id': ids.astype(int), 'event_time': range(2_000_000)})\n",
    "\n",
    "# Load Users (100k rows)\n",
    "try:\n",
    "    df_users = pd.read_parquet('../data/users.parquet')\n",
    "except FileNotFoundError:\n",
    "    df_users = pd.DataFrame({'user_id': range(1, 100001), 'country': 'USA'})\n",
    "\n",
    "print(f\"Loaded Users: {len(df_users):,} rows (The 'Small' Table)\")\n",
    "\n",
    "# SIMULATION CONFIG\n",
    "NUM_NODES = 4\n",
    "\n",
    "def get_node_id(key, num_nodes=NUM_NODES):\n",
    "    \"\"\"Simulates a hash-based sharding function.\"\"\"\n",
    "    # Simple modulo hashing\n",
    "    return key % num_nodes\n",
    "\n",
    "print(\"\\n--- CLUSTER SIMULATION READY ---\")\n",
    "print(f\"Simulating a {NUM_NODES}-Node Cluster.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47e844c",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442d0549",
   "metadata": {},
   "source": [
    "## 2. Experiment 1: The \"Hot Spot\" (Data Skew)\n",
    "**The Concept**: In a distributed system, we split data across nodes using a Sharding Key. A common strategy is `Hash(ID) % NodeCount`. However, if one user has 50% of the data (e.g., Justin Bieber on Twitter), that one node will fill up while the others sit idle. This is Data Skew. The query is only as fast as the slowest node.\n",
    "\n",
    "**The Hypothesis**: \"If we shard `clickstream` by `user_id`, will the work be distributed evenly across our 4 nodes?\"\n",
    "\n",
    "**The Experiment**: We will simulate distributing the rows to 4 lists (Nodes) based on the `user_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d83c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 1: HYPOTHESIS ---\n",
    "# We have 2M rows. Ideally, each node gets 500k rows.\n",
    "# Let's see what actually happens due to the content of the data.\n",
    "\n",
    "# --- STEP 2: RUN EXPERIMENT ---\n",
    "start_time = time.time()\n",
    "\n",
    "# Create 4 empty \"Nodes\"\n",
    "nodes = {i: 0 for i in range(NUM_NODES)}\n",
    "\n",
    "# Simulate the Sharding: Calculate destination node for every row\n",
    "# We use pandas vectorized operations for speed, but the logic is physical sharding\n",
    "destinations = df_clicks['user_id'] % NUM_NODES\n",
    "distribution = destinations.value_counts().sort_index()\n",
    "\n",
    "sharding_time = time.time() - start_time\n",
    "\n",
    "print(f\"Sharding calculation took: {sharding_time:.4f} seconds\")\n",
    "print(\"\\nRow Counts per Node:\")\n",
    "for node_id, count in distribution.items():\n",
    "    print(f\"Node {node_id}: {count:,} rows\")\n",
    "\n",
    "# --- STEP 3: VISUALIZE ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "bars = plt.bar(distribution.index, distribution.values, color=['green', 'green', 'red', 'green'])\n",
    "plt.title('The \"Hot Spot\": Row Distribution per Node')\n",
    "plt.xlabel('Node ID')\n",
    "plt.ylabel('Row Count')\n",
    "plt.xticks(range(NUM_NODES))\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Annotate the skewed node\n",
    "skewed_node_idx = distribution.idxmax()\n",
    "plt.annotate('The Justin Bieber Node\\n(Straggler)', \n",
    "             xy=(skewed_node_idx, distribution[skewed_node_idx]), \n",
    "             xytext=(skewed_node_idx + 0.5, distribution[skewed_node_idx]),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bb76a5",
   "metadata": {},
   "source": [
    "**The Physics**: You should see one bar significantly higher than the others.\n",
    "- **The Straggler Problem**: If you run a query like SELECT COUNT(*) ... GROUP BY user_id, Nodes 0, 2, and 3 will finish quickly and sit idle. Node 1 (with 50% of the data) is still working.\n",
    "- **Implication**: Your expensive 100-node cluster is effectively reduced to the speed of a single node because of a poor choice of Sharding Key on skewed data.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852bc559",
   "metadata": {},
   "source": [
    "## 3. Experiment 2: The Shuffle (The Network Tax)\n",
    "**The Concept**: The most expensive operation in a distributed system is the Shuffle. If Node A needs to join data that lives on Node B, we must send that data over the network.\n",
    "- **Scenario A (Co-Located)**: Data is pre-sorted/partitioned. Node A has all the data it needs. Zero network traffic.\n",
    "- **Scenario B (Shuffle Required)**: Data is random. We must serialize it and send it across the wire to group it.\n",
    "\n",
    "**The Hypothesis**: \"Is it faster to process data that is already locally aligned (No Shuffle) vs. data that needs to be moved (Shuffle)?\"\n",
    "\n",
    "**The Experiment**: We will simulate the \"Physics\" of a shuffle by forcing a memory copy and re-organization of the data, versus a simple pass-through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904d862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 1: PREPARE DATA ---\n",
    "# create a dataset that requires grouping\n",
    "data_size = 1_000_000\n",
    "df_shuffle = pd.DataFrame({\n",
    "    'key': np.random.randint(0, 1000, data_size),\n",
    "    'value': np.random.randn(data_size)\n",
    "})\n",
    "\n",
    "# --- STEP 2: RUN EXPERIMENT ---\n",
    "\n",
    "# Case 1: LOCAL (No Shuffle)\n",
    "# Simulates data already partitioned by Key. \n",
    "# We just groupby locally on the existing chunk.\n",
    "start_local = time.time()\n",
    "# In a sorted/partitioned scenario, the DB just scans and aggregates linearly\n",
    "local_result = df_shuffle.groupby('key')['value'].sum()\n",
    "time_local = time.time() - start_local\n",
    "\n",
    "# Case 2: NETWORK SHUFFLE\n",
    "# Simulates having to redistribute data.\n",
    "# 1. Hash Partition the data (CPU Cost)\n",
    "# 2. \"Move\" data (Memory Copy / Serialization Cost simulated by copy)\n",
    "# 3. GroupBy on new partitions\n",
    "start_shuffle = time.time()\n",
    "\n",
    "# A. Partitioning (The CPU cost of deciding where data goes)\n",
    "df_shuffle['partition'] = df_shuffle['key'] % 4\n",
    "\n",
    "# B. The Exchange (The I/O cost of moving data)\n",
    "# We simulate the network cost by explicitly copying data into new buffers\n",
    "node_buffers = []\n",
    "for i in range(4):\n",
    "    # This copy simulates serialization + network transmission\n",
    "    part = df_shuffle[df_shuffle['partition'] == i].copy()\n",
    "    node_buffers.append(part)\n",
    "\n",
    "# C. Final Aggregation (The Receive side)\n",
    "final_results = []\n",
    "for part in node_buffers:\n",
    "    final_results.append(part.groupby('key')['value'].sum())\n",
    "\n",
    "time_shuffle = time.time() - start_shuffle\n",
    "\n",
    "# --- STEP 3: VISUALIZE ---\n",
    "times = [time_local, time_shuffle]\n",
    "labels = ['Local Compute\\n(No Shuffle)', 'Distributed Shuffle\\n(Network Tax)']\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(labels, times, color=['blue', 'orange'])\n",
    "plt.title('Cost of The Shuffle: Local vs. Networked')\n",
    "plt.ylabel('Execution Time (s)')\n",
    "for i, v in enumerate(times):\n",
    "    plt.text(i, v, f\"{v:.4f}s\", ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5adae92",
   "metadata": {},
   "source": [
    "**The Physics**:\n",
    "- **CPU is fast, I/O is slow**. Even though we only simulated memory copies (not actual network latency), the \"Shuffle\" is significantly slower.\n",
    "- **Serialization**: In a real cluster, the Shuffle involves pickling data, putting it in a TCP packet, sending it over a wire, and unpickling it.\n",
    "- **Takeaway**: A \"Broadcast Join\" or good \"Data Modeling\" tries to eliminate this orange bar.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3777e10c",
   "metadata": {},
   "source": [
    "## 4. Experiment 3: Broadcast vs. Shuffle Join\n",
    "**The Concept**: When joining a Large Table (Clickstream) and a Small Table (Users):\n",
    "- **Shuffle Join**: We cut both tables into pieces and shuffle both across the network so keys align. (Moving the Mountain).\n",
    "- **Broadcast Join**: We send a copy of the entire small table to every node. The large table stays put. (Moving the Climber).\n",
    "\n",
    "**The Hypothesis**: \"If the small table fits in RAM, Broadcast will crush Shuffle.\"\n",
    "\n",
    "**The Experiment**: We will use DuckDB to compare a join where we hint for a Broadcast vs. a generic join plan (simulated logic).\n",
    "\n",
    "**Note**: Since DuckDB is single-node, we simulate the 'physics' by comparing the data volume moved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2099ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 1: SETUP ---\n",
    "con = duckdb.connect()\n",
    "con.register('clicks', df_clicks) # 2M Rows\n",
    "con.register('users', df_users)   # 100k Rows\n",
    "\n",
    "# --- STEP 2: MEASURE DATA MOVEMENT ---\n",
    "# In a physical distributed system, the cost is proportional to bytes transmitted.\n",
    "\n",
    "# COST MODEL:\n",
    "# Shuffle Join Cost = Size(Table A) + Size(Table B) (Everything moves)\n",
    "# Broadcast Join Cost = Size(Table B) * N_Nodes      (Only small table moves)\n",
    "\n",
    "size_clicks_mb = df_clicks.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "size_users_mb  = df_users.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "num_nodes = 10  # Let's imagine a 10-node cluster\n",
    "\n",
    "shuffle_cost = size_clicks_mb + size_users_mb\n",
    "broadcast_cost = size_users_mb * num_nodes\n",
    "\n",
    "print(f\"Dataset Sizes:\")\n",
    "print(f\"  Big Table:   {size_clicks_mb:.2f} MB\")\n",
    "print(f\"  Small Table: {size_users_mb:.2f} MB\")\n",
    "print(f\"\\nSimulated Network Traffic (10 Nodes):\")\n",
    "print(f\"  Shuffle Join:   {shuffle_cost:.2f} MB (Moving the Big Table)\")\n",
    "print(f\"  Broadcast Join: {broadcast_cost:.2f} MB (Replicating Small Table)\")\n",
    "\n",
    "# --- STEP 3: VISUALIZE ---\n",
    "costs = [shuffle_cost, broadcast_cost]\n",
    "labels = ['Shuffle Join\\n(Move Big Table)', 'Broadcast Join\\n(Copy Small Table)']\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "bars = plt.bar(labels, costs, color=['red', 'green'])\n",
    "plt.title('Network Traffic: Shuffle vs. Broadcast')\n",
    "plt.ylabel('Data Moved (MB)')\n",
    "\n",
    "# Add Ratio\n",
    "improvement = shuffle_cost / broadcast_cost\n",
    "plt.annotate(f\"{improvement:.1f}x Less Traffic!\", \n",
    "             xy=(1, broadcast_cost), \n",
    "             xytext=(1, shuffle_cost/2),\n",
    "             arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
    "             ha='center')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# --- STEP 4: PROOF IN DUCKDB ---\n",
    "# While DuckDB isn't distributed, we can see how fast the join is \n",
    "# when indexes (hash maps) are built on the small table.\n",
    "\n",
    "print(\"Running Actual Join in DuckDB...\")\n",
    "start_join = time.time()\n",
    "con.execute(\"SELECT count(*) FROM clicks JOIN users ON clicks.user_id = users.user_id\").fetchall()\n",
    "print(f\"Join Time: {time.time() - start_join:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1768d98a",
   "metadata": {},
   "source": [
    "## The Physics:\n",
    "\n",
    "- **The Math**: If BigTable > SmallTable * NodeCount, Broadcast wins.\n",
    "- **The Bottleneck**: Network bandwidth is usually 10Gbps. Memory bandwidth is 50GBps. Moving 30MB (Broadcast) is instant; moving 100GB (Shuffle) takes minutes.\n",
    "- **Spark/Snowflake Tip**: If you see a \"Shuffle Hash Join\" in your query plan for a small lookup table, you are wasting money. Force a Broadcast."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
