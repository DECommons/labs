{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "418650b0",
   "metadata": {},
   "source": [
    "# Module 5: Advanced Indexing\n",
    "**Goal**: Shatter the illusion that \"Indexing Everything\" makes databases faster.\n",
    "\n",
    "In the previous chapter, we learned about the B-Tree—the \"Telephone Book\" of the database world. It is excellent for unique lookups.\n",
    "\n",
    "But what if we aren't looking for a unique ID?\n",
    "1. **What if we are filtering by Gender or Status?** (There are only a few distinct values).\n",
    "2. **What if we are in the Cloud?** (Where B-Trees are too heavy to maintain).\n",
    "3. What does an index cost?** (We pay for read speed with write slowness).\n",
    "\n",
    "In this lab, we will expose the physics of **Bitmaps**, **Write Amplification**, and **Zone Maps**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2712ff",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Preparation\n",
    "First, we establish our connections and prepare the specific files needed for the Zone Map experiment. We will convert our CSVs to Parquet to demonstrate how column stores use metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660e226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import duckdb\n",
    "import psycopg2\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Connect to the embedded OLAP engine (DuckDB)\n",
    "con_duck = duckdb.connect(database=':memory:')\n",
    "\n",
    "# Connect to the OLTP engine (Postgres)\n",
    "# Note: Ensure your Docker container matches these credentials\n",
    "db_params = {\n",
    "    \"host\": \"db_int_opt\",\n",
    "    \"port\": 5432,\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"password\",\n",
    "    \"dbname\": \"db_int_opt\"\n",
    "}\n",
    "\n",
    "def get_pg_connection():\n",
    "    conn = psycopg2.connect(**db_params)\n",
    "    conn.autocommit = True\n",
    "    return conn\n",
    "\n",
    "# PREP WORK: Convert CSVs to Parquet for Section 5.3\n",
    "# We need a Sorted file and a Shuffled file to prove how Zone Maps work.\n",
    "print(\"Converting CSVs to Parquet for the Lab...\")\n",
    "\n",
    "con_duck.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE orders_sorted AS SELECT * FROM read_csv_auto('../data/orders_sorted.csv');\n",
    "    COPY orders_sorted TO '../data/orders_sorted.parquet' (FORMAT PARQUET);\n",
    "    \n",
    "    CREATE OR REPLACE TABLE orders_shuffled AS SELECT * FROM read_csv_auto('../data/orders_shuffled.csv');\n",
    "    COPY orders_shuffled TO '../data/orders_shuffled.parquet' (FORMAT PARQUET);\n",
    "\"\"\")\n",
    "\n",
    "print(\"Setup Complete. Data is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a85c03c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d05a3d7",
   "metadata": {},
   "source": [
    "## 5.1 Bitmap Operations: The Power of Low Cardinality\n",
    "**The Physical Concept**: Standard B-Trees struggle when a column has very Low Cardinality (few unique values, like \"Status: Active/Inactive\"). If 50% of your users are \"Active,\" a B-Tree is useless because it would still return half the table.\n",
    "\n",
    "Instead, databases use **Bitmaps**. Imagine a simple string of bits: `10011`.\n",
    "- User 1 (Active) -> 1\n",
    "- User 2 (Inactive) -> 0\n",
    "- User 3 (Inactive) -> 0\n",
    "\n",
    "Computers can process bitwise operations (AND/OR) on these streams incredibly fast—much faster than comparing Integers or Strings.\n",
    "\n",
    "**The Experiment**: We will use DuckDB (a Column Store that relies heavily on vectorization and compression similar to Bitmaps) to compare the speed of filtering on a Low Cardinality column vs. a High Cardinality column.\n",
    "\n",
    "#### Step 1: Hypothesis\n",
    "\"Will filtering by a Boolean (Low Cardinality) be faster than filtering by a specific User ID (High Cardinality) even if they return the same amount of data?\"\n",
    "\n",
    "### Step 2: The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12d1769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 100k Users data\n",
    "con_duck.execute(\"CREATE OR REPLACE TABLE users AS SELECT * FROM read_parquet('../data/users.parquet')\")\n",
    "\n",
    "# We want to select approximately the same number of rows to make it a fair fight.\n",
    "# Let's see how many users are 'Active' (Low Cardinality)\n",
    "count = con_duck.execute(\"SELECT COUNT(*) FROM users WHERE account_status = 'Active'\").fetchone()[0]\n",
    "print(f\"Target Row Count: {count}\")\n",
    "\n",
    "# 1. Low Cardinality Filter (Account Status)\n",
    "start_low = time.time()\n",
    "for _ in range(50): # Run 50 times to amplify the CPU effect\n",
    "    con_duck.execute(\"SELECT count(*) FROM users WHERE account_status = 'Active'\").fetchall()\n",
    "end_low = time.time()\n",
    "avg_low = (end_low - start_low) / 50\n",
    "\n",
    "# 2. High Cardinality Filter (User ID)\n",
    "# We filter for a range of IDs that roughly equals the count of 'Active' users to simulate scanning\n",
    "start_high = time.time()\n",
    "for _ in range(50):\n",
    "    con_duck.execute(f\"SELECT count(*) FROM users WHERE user_id < {count}\").fetchall()\n",
    "end_high = time.time()\n",
    "avg_high = (end_high - start_high) / 50\n",
    "\n",
    "print(f\"Low Cardinality Time: {avg_low:.6f}s\")\n",
    "print(f\"High Cardinality Time: {avg_high:.6f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d996a0",
   "metadata": {},
   "source": [
    "#### Step 3: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165c3d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(['Low Cardinality (Status)', 'High Cardinality (ID)'], [avg_low, avg_high], color=['green', 'red'])\n",
    "plt.title('Filter Performance: Bitwise vs. Integer Comparison')\n",
    "plt.ylabel('Execution Time (seconds)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2dbfcf",
   "metadata": {},
   "source": [
    "#### Step 4: Analysis\n",
    "**Why was the Low Cardinality query faster?** In a column store, low cardinality data is often stored using Dictionary Encoding or Run-Length Encoding. The engine doesn't scan the text \"Active\"; it scans a compressed list of small integers or bits.\n",
    "- CPU Efficiency: The CPU can process 64 \"Active/Inactive\" flags in a single cycle using SIMD (Single Instruction, Multiple Data) instructions.\n",
    "- High Cardinality: Comparing unique Integers (`user_id < 50000`) requires more complex logic for every single row, consuming more CPU cycles.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a59e291",
   "metadata": {},
   "source": [
    "## 5.2 The Cost of Indexes: Write Amplification\n",
    "**The Physical Concept**: New engineers often say, \"My query is slow? I'll just add an index!\" But Indexes are not free magic. An index is a copy of your data, sorted in a specific way. Every time you `INSERT` a row into a table, the database must also `INSERT` that entry into every single index attached to that table. This is called Write Amplification.\n",
    "\n",
    "**The Experiment**: We will use Postgres (OLTP) to insert 50,000 rows into two tables:\n",
    "1. Light Table: 0 Indexes.\n",
    "2. Heavy Table: 4 Indexes.\n",
    "\n",
    "#### Step 1: Hypothesis\n",
    "\"How much slower will the Heavy Table be? 2x? 10x?\"\n",
    "\n",
    "#### Step 2: The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e444552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# 5.2 Final: The \"Firehose\" Test (COPY Protocol)\n",
    "\n",
    "pg_conn = get_pg_connection()\n",
    "pg_conn.autocommit = True # We want raw speed, let the DB manage internal locking\n",
    "cur = pg_conn.cursor()\n",
    "\n",
    "# 1. Reset Tables\n",
    "cur.execute(\"DROP TABLE IF EXISTS write_test_light;\")\n",
    "cur.execute(\"CREATE TABLE write_test_light (id SERIAL PRIMARY KEY, uuid TEXT, data TEXT, created_at TIMESTAMP);\")\n",
    "\n",
    "cur.execute(\"DROP TABLE IF EXISTS write_test_heavy;\")\n",
    "cur.execute(\"CREATE TABLE write_test_heavy (id SERIAL PRIMARY KEY, uuid TEXT, data TEXT, created_at TIMESTAMP);\")\n",
    "\n",
    "# 2. Add 4 Indexes to the Heavy table\n",
    "cur.execute(\"CREATE INDEX idx_heavy_uuid ON write_test_heavy(uuid);\")\n",
    "cur.execute(\"CREATE INDEX idx_heavy_data ON write_test_heavy(data);\")\n",
    "cur.execute(\"CREATE INDEX idx_heavy_created ON write_test_heavy(created_at);\")\n",
    "cur.execute(\"CREATE INDEX idx_heavy_composite ON write_test_heavy(uuid, created_at);\")\n",
    "\n",
    "# 3. Prepare Data in Memory (CSV Format)\n",
    "# We use 100k rows now because COPY is so fast, 50k might be too quick to measure!\n",
    "print(\"Generating 100k rows of CSV data in memory...\")\n",
    "csv_buffer = io.StringIO()\n",
    "# Format: uuid, data, created_at (id is auto-generated so we skip it in copy)\n",
    "for x in range(100000):\n",
    "    csv_buffer.write(f\"uuid-{x}\\tdata-{x}\\t2023-01-01\\n\")\n",
    "csv_buffer.seek(0)\n",
    "\n",
    "print(\"Starting Firehose (COPY)...\")\n",
    "\n",
    "# 4. Measure Write Speed: Light Table\n",
    "start_light = time.time()\n",
    "csv_buffer.seek(0) # Rewind buffer\n",
    "# COPY is atomic and optimized for throughput\n",
    "cur.copy_from(csv_buffer, 'write_test_light', columns=('uuid', 'data', 'created_at'))\n",
    "end_light = time.time()\n",
    "\n",
    "# 5. Measure Write Speed: Heavy Table\n",
    "start_heavy = time.time()\n",
    "csv_buffer.seek(0) # Rewind buffer\n",
    "cur.copy_from(csv_buffer, 'write_test_heavy', columns=('uuid', 'data', 'created_at'))\n",
    "end_heavy = time.time()\n",
    "\n",
    "pg_conn.close()\n",
    "\n",
    "time_light = end_light - start_light\n",
    "time_heavy = end_heavy - start_heavy\n",
    "\n",
    "print(f\"Light Table (No Index): {time_light:.4f}s\")\n",
    "print(f\"Heavy Table (4 Indexes): {time_heavy:.4f}s\")\n",
    "print(f\"Write Amplification Factor: {time_heavy / time_light:.1f}x Slower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769bc612",
   "metadata": {},
   "source": [
    "#### Step 3: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0e3814",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=['No Extra Indexes', '4 Extra Indexes'], y=[time_light, time_heavy], palette='magma')\n",
    "plt.title('The Cost of Indexes: Write Amplification')\n",
    "plt.ylabel('Time to Insert 50k Rows (s)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80997e3d",
   "metadata": {},
   "source": [
    "#### Step 4: Analysis\n",
    "You likely saw a massive slowdown (often 5x-10x).\n",
    "- **Physics**: When you wrote to write_test_light, Postgres appended data to the Heap (the main storage). Simple.\n",
    "- **The Burden**: When you wrote to write_test_heavy, Postgres had to:\n",
    "    1. Write to the Heap.\n",
    "    2. Traverse the B-Tree for `uuid` to find the leaf node.\n",
    "    3. Traverse the B-Tree for `data`.\n",
    "    4. Traverse the B-Tree for `created_at`.\n",
    "    5. Traverse the B-Tree for the composite index.\n",
    "- **Page Splits**: If an index page was full, Postgres had to split the page, move data around, and rebalance the tree. This is pure I/O overhead.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
